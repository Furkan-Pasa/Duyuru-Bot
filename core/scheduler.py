# core/scheduler.py
"""
APScheduler kullanarak gÃ¶rev zamanlamasÄ± (Orkestrasyon).

Bu modÃ¼l, `DuyuruScheduler` sÄ±nÄ±fÄ±nÄ± iÃ§erir. Bu sÄ±nÄ±fÄ±n sorumluluklarÄ±:
1. `bot_config`'i okuyarak tÃ¼m aktif scraper'larÄ± dinamik olarak yÃ¼klemek.
2. `APScheduler`'Ä± baÅŸlatmak ve her scraper iÃ§in 2 gÃ¶rev kurmak:
    - Biri 'cron' (periyodik) gÃ¶rev (Ã¶rn: her saat '01' ve '31'de).
    - Biri 'date' (ilk Ã§alÄ±ÅŸtÄ±rma) gÃ¶revi (bot baÅŸlar baÅŸlamaz).
3. `_run_check` metodu ile scraper'larÄ± tetiklemek, DB'yi kontrol etmek.
4. Yeni/gÃ¼ncel duyurularÄ± `telegram_bot`'a gÃ¶ndermek.
5. `shutdown` ile tÃ¼m kaynaklarÄ± (DB, session'lar, thread'ler) gÃ¼venle kapatmak.
"""

import importlib
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from apscheduler.triggers.cron import CronTrigger
from apscheduler.schedulers.background import BackgroundScheduler

import bot_config
from core.database import Database
from scrapers.base_scraper import BaseScraper
from core.telegram_bot import send_to_telegram, start_telegram_loop, stop_telegram_loop
from core.logger import (
    log_debug, log_info, log_error, log_warning, log_scraper_start, 
    log_task_finish, log_new_announcement, log_critical 
)

def import_from_string(path: str):
    """
    'paket.modul.ClassAdi' formatÄ±ndaki bir yolu dinamik olarak import eder.
    
    Ã–rnek:
        'scrapers.BSEU_Duyuru.Scraper1' -> Scraper1 class'Ä±nÄ± dÃ¶ndÃ¼rÃ¼r.
        
    Returns:
        YÃ¼klenen Class nesnesi.
    """
    try:
        # Yolu, modÃ¼l ve class adÄ± olarak ayÄ±rÄ±r
        # (Ã¶rn: 'scrapers.BSEU_Duyuru', 'Scraper1')
        module_path, class_name = path.rsplit('.', 1)
    except ValueError:
        log_error(f"âŒ GeÃ§ersiz import yolu: '{path}'. Format: 'paket.modul.ClassAdi'")
        raise
    
    # ModÃ¼lÃ¼ import et
    module = importlib.import_module(module_path)
    
    # Class'Ä± modÃ¼lden al
    try:
        ScraperClass = getattr(module, class_name)
        return ScraperClass
    except AttributeError:
        log_error(f"âŒ '{module_path}' modÃ¼lÃ¼nde '{class_name}' sÄ±nÄ±fÄ± bulunamadÄ±.")
        raise

class DuyuruScheduler:
    """
    Scraper'larÄ± yÃ¼kler, gÃ¶revleri zamanlar ve bot dÃ¶ngÃ¼sÃ¼nÃ¼ yÃ¶netir.
    """
    def __init__(self):
        """
        Scheduler'Ä±, veritabanÄ±nÄ± ve scraper'larÄ± hazÄ±rlar.
        Async Telegram loop'unu (ayrÄ± thread'de) baÅŸlatÄ±r.
        """
        log_debug("â³ Scheduler baÅŸlatÄ±lÄ±yor...")
        self.scheduler = BackgroundScheduler(timezone="Europe/Istanbul")
        self.db = Database()
        
        # Async iÅŸlemleri yÃ¶netecek arkaplan thread'ini baÅŸlat
        start_telegram_loop()
        
        # Aktif scraper instance'larÄ±nÄ± tutar
        self.scrapers = {}
        self._load_scrapers()

    def _load_scrapers(self):
        """
        `bot_config.SITES` listesini okur, 'enabled' olan scraper'larÄ±
        dinamik olarak import eder ve `self.scrapers`'a yÃ¼kler.
        """
        log_info("â³ Scheduler | Scraper'lar yÃ¼kleniyor...")
        
        for site_config in bot_config.SITES:
            # Pasif siteleri atla
            if not site_config.get('enabled', False):
                log_info(f"âš ï¸ {site_config['name']} pasif, atlanÄ±yor.")
                continue
                
            try:
                site_url = site_config.get('url')
                site_name = site_config['name']
                scraper_path = site_config.get('scraper_path')
                
                # Config kontrolleri
                if not site_url:
                    log_error(f"âŒ {site_name} iÃ§in config'de 'url' bulunamadÄ±. AtlanÄ±yor.")
                    continue
                if not scraper_path:
                    log_error(f"âŒ {site_name} iÃ§in config'de 'scraper_path' eksik. AtlanÄ±yor.")
                    continue
                
                # 'scrapers.BSEU_Duyuru.Scraper1' yolundan Scraper1 class'Ä±nÄ± import et
                ScraperClass = import_from_string(scraper_path)
                
                # Scraper'dan bir instance oluÅŸtur (BaseScraper __init__ Ã§aÄŸrÄ±lÄ±r)
                scraper_instance: BaseScraper = ScraperClass(url=site_url, name=site_name)
                
                # HazÄ±r scraper'Ä± sÃ¶zlÃ¼ÄŸe ekle
                self.scrapers[site_name] = {
                    'instance': scraper_instance,
                    'config': site_config
                }
                log_debug(f"âœ… Scheduler | Scraper hazÄ±r: [{site_name}]")
                
            except ImportError as e:
                log_error(f"âŒ Scheduler | Scraper import hatasÄ± ({site_config['name']}): {e}", exc_info=True)
            except Exception as e:
                log_error(f"âŒ  Scheduler | Scraper yÃ¼klenemedi ({site_config['name']}): {e}", exc_info=True)

    def start(self):
        """
        ZamanlayÄ±cÄ±yÄ± yapÄ±landÄ±rÄ±r, gÃ¶revleri ekler ve arkaplan thread'inde baÅŸlatÄ±r.
        
        Her site iÃ§in 2 gÃ¶rev ekler:
        1. `CronTrigger`: `config.py`'deki dakikalarda (Ã¶rn: '01', '31') her saat Ã§alÄ±ÅŸÄ±r.
        2. `date`: Bot baÅŸlar baÅŸlamaz "bir kerelik" Ã§alÄ±ÅŸÄ±r. (Staggered)
        """
        if not self.scrapers:
            log_warning("âš ï¸ Scheduler | HiÃ§ aktif scraper bulunamadÄ±. LÃ¼tfen config.py dosyasÄ±nÄ± kontrol edin.")
            return

        log_info("â° Scheduler | ZamanlayÄ±cÄ± gÃ¶revleri ayarlanÄ±yor...")
        
        # Botun ilk gÃ¶revini Ã§alÄ±ÅŸtÄ±rmasÄ± iÃ§in baÅŸlangÄ±Ã§ gecikmesi (saniye)
        # GÃ¶revlerin birbirini boÄŸmamasÄ± iÃ§in 'stagger' (kademeli baÅŸlatma) yapÄ±lÄ±r
        initial_run_delay_seconds = 2
        
        for site_name, data in self.scrapers.items():
            site_config = data['config']
            scraper_instance = data['instance']
        
            # --- GÃ–REV 1: DÃœZENLÄ° (CRON) GÃ–REVÄ° ---
            # Bu, config'de belirtilen dakikalarda (Ã¶rn: '01', '31') her saat Ã§alÄ±ÅŸÄ±r
            minutes = ','.join(site_config['schedule_minutes'])
            self.scheduler.add_job(
                self._run_check,
                trigger=CronTrigger(minute=minutes, hour='*'),
                args=[site_name, scraper_instance, site_config],
                name=f"Check_{site_name}",
                misfire_grace_time=120  # GÃ¶rev gecikirse 120 saniyeye kadar yine Ã§alÄ±ÅŸtÄ±r
            )
            log_info(f"ğŸ“… Scheduler | GÃ¶rev eklendi: [{site_name}] (Her saatin {minutes} dakikalarÄ±nda)")

            # --- GÃ–REV 2: Ä°LK Ã‡ALIÅTIRMA (DATE) GÃ–REVÄ° ---
            # Bot baÅŸladÄ±ktan 'initial_run_delay_seconds' saniye sonra "bir kerelik" Ã§alÄ±ÅŸÄ±r.
            run_time = datetime.now() + timedelta(seconds=initial_run_delay_seconds)
            
            self.scheduler.add_job(
                self._run_check,
                trigger='date', # 'date' trigger'Ä± "bir kerelik" demektir
                run_date=run_time,
                args=[site_name, scraper_instance, site_config],
                name=f"InitialRun_{site_name}"
            )
            log_debug(f"ğŸš€ Scheduler | Ä°lk-Ã§alÄ±ÅŸtÄ±rma gÃ¶revi eklendi: [{site_name}] (Ã‡alÄ±ÅŸma zamanÄ±: {run_time.strftime('%H:%M:%S')})")
            
            # Bir sonraki scraper'Ä±n ilk Ã§alÄ±ÅŸtÄ±rmasÄ± 5 saniye sonra olsun
            initial_run_delay_seconds += 5
            
        # TÃ¼m gÃ¶revler eklendi, ÅŸimdi scheduler'Ä± BAÅLAT
        self.scheduler.start()
        log_info("â³ Scheduler | ZamanlayÄ±cÄ± BAÅLATILDI. GÃ¶revler arkaplanda Ã§alÄ±ÅŸacak.")

    def shutdown(self, from_start_loop: bool = False):
        """
        Scheduler'Ä± ve kaynaklarÄ± (DB, session'lar, async loop) gÃ¼venli bir ÅŸekilde kapatÄ±r.
        
        `main.py`'deki signal_handler (CTRL+C) tarafÄ±ndan Ã§aÄŸrÄ±lÄ±r.
        
        Args:
            from_start_loop (bool): KapatmanÄ±n `main.py`'deki ana dÃ¶ngÃ¼den
                                    gelip gelmediÄŸini belirtir (normalde False).
        """
        log_debug("â³ Scheduler | GÃ¼venli kapatma baÅŸlatÄ±ldÄ±...")
        
        # 1. APScheduler'Ä± kapat (yeni gÃ¶rev almayÄ± durdur, Ã§alÄ±ÅŸanlarÄ± bitir)
        try:
            if self.scheduler.running:
                self.scheduler.shutdown(wait=True)
                log_debug("ğŸ”’ Scheduler | APScheduler gÃ¶revleri durduruldu.")
        except Exception as e:
            log_error(f"ğŸ›‘ Scheduler kapatÄ±lÄ±rken hata: {e}")

        # 2. Async Telegram thread'ini kapat
        stop_telegram_loop()
        
        # 3. VeritabanÄ± baÄŸlantÄ±larÄ±nÄ± (thread-local) kapat
        try:
            self.db.close()
        except Exception as e:
            log_error(f"ğŸ›‘ VeritabanÄ± kapatÄ±lÄ±rken hata: {e}")
        
        # 4. TÃ¼m scraper'larÄ±n (BaseScraper) 'requests.Session'larÄ±nÄ± kapat
        log_debug("â³ Scheduler | Scraper session'larÄ± kapatÄ±lÄ±yor...")
        for site_name, data in self.scrapers.items():
            try:
                data['instance'].close()
            except Exception as e:
                log_error(f"ğŸ›‘ [{site_name}] scraper session kapatÄ±lÄ±rken hata: {e}")
                
        log_info("ğŸ”’ Scheduler | TÃ¼m kaynaklar serbest bÄ±rakÄ±ldÄ±. KapatÄ±ldÄ±.")
        
        # EÄŸer bu Ã§aÄŸrÄ± main.py'den (signal_handler) geliyorsa, main.py'deki sys.exit() kapatmayÄ± tamamlayacaktÄ±r.
        if from_start_loop:
            import sys
            sys.exit(0)
                    
    def _run_check(self, site_name: str, scraper: BaseScraper, site_config: Dict):
        """
        Bir site iÃ§in kontrol iÅŸlemini yÃ¼rÃ¼ten ana fonksiyon (APScheduler bunu tetikler).
        
        1. `scraper.scrape()` ile duyuru listesini Ã§eker.
        2. DB'de bu site iÃ§in kayÄ±t olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.
        3. DB boÅŸsa (`total_in_db == 0`): `_process_first_run`'Ä± Ã§alÄ±ÅŸtÄ±rÄ±r.
        4. DB doluysa: `_process_normal_run`'Ä± Ã§alÄ±ÅŸtÄ±rÄ±r.
        5. Sonucu 'stats' tablosuna kaydeder.
        """
        log_scraper_start(site_name)
        new_count = 0
        updated_count = 0
        
        try:
            # 1. Scrape et (Sadece liste geliyor)
            announcements = scraper.scrape()
            
            if not announcements:
                log_warning(f"âš ï¸ Scheduler | [{site_name}]: Sayfada duyuru bulunamadÄ±.")
                return

            # 2. Bu site iÃ§in ilk Ã§alÄ±ÅŸtÄ±rma mÄ±?
            total_in_db = self.db.get_total_announcements(site_name=site_name)
            
            if total_in_db == 0 and announcements:
                # --- Ä°LK Ã‡ALIÅTIRMA MANTIÄI ---
                log_info(f"âœ¨ [{site_name}]: VeritabanÄ± boÅŸ. Ä°lk Ã§alÄ±ÅŸtÄ±rma ayarlanÄ±yor.")
                new_count = self._process_first_run(scraper, site_config, announcements)
            else:
                # --- NORMAL Ã‡ALIÅTIRMA MANTIÄI ---
                new_count, updated_count = self._process_normal_run(scraper, site_config, announcements)

            # --- GÃ–REV TAMAMLANDI ---
            log_task_finish(site_name, new_count, updated_count)
            
        # _run_check'in Ã§Ã¶kmemesi kritik, bu yÃ¼zden broad-except
        except Exception as e:
            log_critical(f"ğŸš¨ [{site_name}] GÃ–REVÄ°NDE BÃœYÃœK HATA: {e}", exc_info=True)
                
    def _process_first_run(self, scraper: BaseScraper, site_config: Dict, announcements: List[Dict]) -> int:
        """
        Ä°lk Ã§alÄ±ÅŸtÄ±rma: DB'yi spam'sÄ±z doldurma.
        
        Scraper'dan gelen tÃ¼m duyurularÄ± alÄ±r.
        (FIRST_RUN_FETCH_LIMIT) kadarÄ±nÄ± db ekler.
        Returns:
            GÃ¶nderilen duyuru sayÄ±sÄ± (int).
        """
        limit_to_save = bot_config.FIRST_RUN_FETCH_LIMIT
        limit_to_send = bot_config.FIRST_RUN_SEND_LIMIT
        
        # 'announcements' listesi en yeniden eskiye sÄ±ralÄ± varsayÄ±lÄ±yor.
        # Sadece kaydetmek istediÄŸimiz kadarÄ±nÄ± al
        announcements_to_save = announcements[:limit_to_save]
        
        total_found = len(announcements)
        total_to_save = len(announcements_to_save)
        
        log_info(f"âœ¨ [{scraper.name}] Sayfada {total_found} duyuru bulundu. Sadece en yeni {total_to_save} tanesi DB'ye ekleniyor...")
        
        for ann in announcements_to_save:
            try:
                # content_text ham HTML veya None olabilir
                content_text = scraper.fetch_announcement_content(ann['url'])
                ann['content'] = content_text
                self.db.save_announcement(scraper.name, ann)
            except Exception as e:
                log_error(f"âŒ [{scraper.name}] (Ä°lk Ã‡alÄ±ÅŸtÄ±rma) Kaydetme/Ã‡ekme hatasÄ±: {e} - URL: {ann['url']}")
                
        log_info(f"âœ¨ [{scraper.name}]: Toplam {total_to_save} duyuru DB'ye eklendi. En son {limit_to_send} tanesi gÃ¶nderiliyor.")
        
        sent_count = 0
    
        # GÃ¶nderilecekler, 'announcements_to_save' listesinin iÃ§inden ilk 'limit_to_send' kadar olmalÄ±
        # reversed() kullanÄ±yoruz ki (eÄŸer 1'den fazla gÃ¶nderilecekse) eskiden yeniye gitsin.
        for ann in reversed(announcements_to_save[:limit_to_send]):
            send_to_telegram(
                channel_id=site_config['telegram_channel_id'],
                site_name=scraper.name,
                announcement=ann,
                message_type='new'
            )
            sent_count += 1
            
        return sent_count     
    
    def _process_normal_run(self, scraper: BaseScraper, site_config: Dict, announcements: List[Dict]) -> Tuple[int, int]:
        """
        Normal Ã§alÄ±ÅŸtÄ±rma: Yeni/gÃ¼ncel duyurularÄ± kontrol eder.
        
        Liste ters Ã§evrilir (eskiden yeniye) ve DB ile karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r:
        1. DB'de yoksa -> YENÄ° (fetch_content, save, send).
        2. DB'de varsa -> GÃœNCELLEME KONTROLÃœ (_check_announcement_update).
        
        Returns:
            (yeni_sayÄ±sÄ±, gÃ¼ncel_sayÄ±sÄ±) tuple'Ä±.
        """
        new_announcements_count = 0
        updated_announcements_count = 0
        site_name = scraper.name
        
        # 1. Listeyi ters Ã§evir (eskiden yeniye doÄŸru kontrol etmek iÃ§in)
        reversed_ann_list = list(reversed(announcements))

        # 2. Config'den TOPLAM limiti al
        total_check_limit = bot_config.NORMAL_RUN_TOTAL_CHECK_LIMIT
        
        # 3. Eskiden-yeniye listenin SADECE son 'X' tanesini al
        announcements_to_check = reversed_ann_list[-total_check_limit:]
        
        # 4. DÃ¶ngÃ¼ ve hash hesabÄ± iÃ§in limitleri ayarla
        total_being_checked = len(announcements_to_check)
        recent_announcements_limit = bot_config.NORMAL_RUN_UPDATE_CHECK_LIMIT
        
        log_debug(f"[{site_name}] Normal Ã§alÄ±ÅŸtÄ±rma: Sayfadaki {len(announcements)} duyurudan {total_being_checked} tanesi (son {recent_announcements_limit} tanesinin iÃ§eriÄŸi) kontrol edilecek.")
        
        # 5. ArtÄ±k 'reversed_ann_list' yerine 'announcements_to_check' listesi Ã¼zerinde dÃ¶n
        for index, ann in enumerate(announcements_to_check):
            ann_id = ann.get('id')
            if not ann_id:
                log_warning(f"âš ï¸ [{site_name}]: ID'siz duyuru bulundu, atlanÄ±yor: {ann.get('title')}")
                continue
            
            # DB'de bu ID var mÄ±?
            db_record = self.db.get_announcement_by_id(site_name, ann_id)
            
            if db_record is None:
                # --- DURUM 1: YENÄ° DUYURU ---
                # NOT: Bu durum, 'NORMAL_RUN_TOTAL_CHECK_LIMIT' ayarÄ±
                # 'FIRST_RUN_FETCH_LIMIT' ayarÄ±ndan kÃ¼Ã§Ã¼k olduÄŸu sÃ¼rece
                # (ve site 10 duyurudan fazlasÄ±nÄ± birden yayÄ±nlamadÄ±ÄŸÄ± sÃ¼rece)
                # "eski" duyurular iÃ§in tetiklenmemelidir.
                
                log_new_announcement(site_name, ann.get('title', 'BaÅŸlÄ±ksÄ±z'))
                try:
                    # Yeni duyurunun iÃ§eriÄŸini Ã§ek
                    content_text = scraper.fetch_announcement_content(ann['url'])
                    ann['content'] = content_text
                except Exception as e:
                    log_error(f"âŒ [{site_name}] (Yeni) Ä°Ã§erik Ã§ekilemedi: {ann['url']}, hata: {e}")
                    ann['content'] = None # Ä°Ã§erik olmasa da baÅŸlÄ±k hash'i ile kaydet
                
                # DB'ye kaydet (eÄŸer zaten eklenmediyse True dÃ¶ner)
                # save_announcement None iÃ§eriÄŸi ve baÅŸlÄ±ktan hash'i yÃ¶netir
                if self.db.save_announcement(site_name, ann):
                    send_to_telegram(
                        channel_id=site_config['telegram_channel_id'],
                        site_name=site_name,
                        announcement=ann,
                        message_type='new'
                    )
                    new_announcements_count += 1
            
            else:
                # --- DURUM 2: MEVCUT DUYURU (GÃ¼ncelleme kontrolÃ¼) ---
                
                # Optimizasyon: Sadece sayfadaki "en yeni Y" duyurunun iÃ§eriÄŸini kontrol et
                is_recent = (index >= total_being_checked - recent_announcements_limit)
                is_updated, updated_content = self._check_announcement_update(scraper, ann, db_record, is_recent)
                
                if is_updated:
                    ann['content'] = updated_content # Mesaj iÃ§in iÃ§eriÄŸi (None veya str) ekle
                    
                    # DB'deki hash'i gÃ¼ncelle
                    new_hash = self.db.generate_hash(
                        updated_content, 
                        fallback_text=ann['title']
                    )
                    
                    # DB'yi (hem hash hem raw_content ile) gÃ¼ncelle
                    self.db.update_announcement(
                        site_name=site_name, 
                        announcement_id=ann_id, 
                        new_title=ann['title'], 
                        new_hash=new_hash,
                        new_raw_content=updated_content # Bu None veya ham HTML olabilir
                    )
                    
                    send_to_telegram(
                        channel_id=site_config['telegram_channel_id'],
                        site_name=site_name,
                        announcement=ann,
                        message_type='update'
                    )
                    updated_announcements_count += 1
                    
        return new_announcements_count, updated_announcements_count    

    def _check_announcement_update(self, scraper: BaseScraper, ann: Dict, db_record: Dict, is_recent: bool) -> Tuple[bool, Optional[str]]:
        """
        Bir duyurunun baÅŸlÄ±ÄŸÄ±nÄ±n veya iÃ§eriÄŸinin gÃ¼ncellenip gÃ¼ncellenmediÄŸini kontrol eder.
        
        Optimizasyon: 
        Ä°Ã§erik (hash) kontrolÃ¼ sadece 'is_recent' (Ã¶rn: son X) ise yapÄ±lÄ±r. 
        BaÅŸlÄ±k kontrolÃ¼ her zaman yapÄ±lÄ±r.
        
        Returns: 
            (bool: GÃ¼ncelleme bulundu mu?, Optional[str]: Ã‡ekilen iÃ§erik (eÄŸer Ã§ekildiyse veya None))
        """
        new_title = ann['title']
        content_text: Optional[str] = None
        update_found = False

        # 1. BaÅŸlÄ±k kontrolÃ¼ (Her zaman, HTTP isteÄŸi gerekmez)
        if new_title != db_record['title']:
            log_info(f"ğŸ”„ [{scraper.name}] BAÅLIK GÃœNCELLENDÄ°: {ann['id']}")
            update_found = True
        
        # 2. "Son X" kuralÄ±: Sadece son X ise Ä°Ã‡ERÄ°K kontrolÃ¼ yap (HTTP isteÄŸi)
        if is_recent:
            try:
                log_debug(f"ğŸŒ [{scraper.name}] (Son {bot_config.NORMAL_RUN_UPDATE_CHECK_LIMIT}) Ä°Ã§erik kontrolÃ¼ yapÄ±lÄ±yor: {ann['id']}")
                content_text = scraper.fetch_announcement_content(ann['url'])
                
                # Ä°Ã§erik None ise baÅŸlÄ±ÄŸÄ± fallback olarak kullanarak hash'le
                new_hash = self.db.generate_hash(
                    content_text,
                    fallback_text=new_title # BaÅŸlÄ±ÄŸÄ±n yenisini kullan
                )
                
                if new_hash != db_record['content_hash']:
                    log_info(f"ğŸ”„ [{scraper.name}] Ä°Ã‡ERÄ°K GÃœNCELLENDÄ°: {ann['id']}")    
                    update_found = True
                    
            except Exception as e:
                log_debug(f"âŒ [{scraper.name}] (Son {bot_config.NORMAL_RUN_UPDATE_CHECK_LIMIT}) Ä°Ã§erikten birisi Ã§ekilemedi: {ann['url']}")

        # 3. GÃ¼ncelleme BAÅLIKTA bulunduysa, ama 'is_recent' olmadÄ±ÄŸÄ± iÃ§in
        # iÃ§erik henÃ¼z Ã§ekilmediyse, Telegram'a gÃ¶ndermek iÃ§in iÃ§eriÄŸi ÅŸimdi Ã§ek.
        if update_found and content_text is None and not is_recent:
            try:
                log_debug(f"ğŸŒ [{scraper.name}] (BaÅŸlÄ±k gÃ¼nc.) Ä°Ã§erik Ã§ekiliyor: {ann['id']}")
                content_text = scraper.fetch_announcement_content(ann['url'])
            except Exception as e:
                log_error(f"ğŸ›‘ [{scraper.name}] (BaÅŸlÄ±k GÃ¼nc.) Ä°Ã§erik Ã§ekilemedi: {ann['url']}, hata: {e}")
                content_text = None  # Hata durumunda None
        
        return update_found, content_text
