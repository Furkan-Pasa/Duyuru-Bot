# scrapers/BSEU_Duyuru.py
"""
Bilecik Ãœniversitesi (BÅEÃœ) 'Liste GÃ¶rÃ¼nÃ¼mÃ¼' Scraper'Ä±.

`BaseScraper`'Ä± implemente eder. 
BÅEÃœ'nÃ¼n '.../arama/4' (Duyurular) formatÄ±ndaki tÃ¼m siteleriyle uyumludur.
(Ã–rn: Bilgisayar MÃ¼h., MÃ¼hendislik Fak., SKS)
"""

import time
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from urllib.parse import urljoin  # GÃ¶receli linkleri tam linke Ã§evirmek iÃ§in
import bot_config
from .base_scraper import BaseScraper
from core.logger import log_debug, log_info, log_warning, log_error 

class Scraper1(BaseScraper):
    """
    BÅEÃœ 'Liste GÃ¶rÃ¼nÃ¼mÃ¼' (arama/4) sayfalarÄ± iÃ§in scraper.
    
    `BaseScraper`'dan 'parse_announcements' ve 'fetch_announcement_content'
    metotlarÄ±nÄ± (o siteye Ã¶zel) implemente eder.
    """
    
    # GÃ¶receli URL'leri (Ã¶rn: /bilgisayar/Icerik/...) tam URL'e Ã§evirmek iÃ§in
    BASE_URL = 'https://www.bilecik.edu.tr'

    def __init__(self, url: str, name: str):
        """
        'BaseScraper'Ä±n __init__'ini Ã§aÄŸÄ±rÄ±r ve bu scraper'Ä±n yÃ¼klendiÄŸini loglar.
        """
        # BaseScraper'a URL ve ismi iletiyor
        super().__init__(url=url, name=name)
        log_info(f"âœ… {self.name} scraper yÃ¼klendi. URL: {self.url}")


    def fetch_announcement_content(self, url: str) -> Optional[str]:
        """
        [IMPLEMENTS BaseScraper]
        Tek bir duyuru URL'ine giderek hash'lenecek ana iÃ§eriÄŸi (HTML) Ã§eker.
        
        1. 'icerik-govde' iÃ§indeki 'icerik-govde'yi (iÃ§ div) Ã¶ncelikli arar.
        2. Ä°Ã§erik bulunamazsa (resim, yÃ¶nlendirme vb.) 'None' dÃ¶ndÃ¼rÃ¼r.
        """
        try:
            time.sleep(bot_config.REQUEST_DELAY_MS / 1000.0)
            log_debug(f"ğŸŒ [{self.name}] Duyuru Ä°Ã§eriÄŸi Ã§ekiliyor: {url}")
            response = self.session.get(url, timeout=20)

            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # 1. Ã–nce iÃ§-iÃ§e (spesifik) olan div'i ara
            content_div = soup.select_one('div.icerik-govde div.icerik-govde')
            if content_div:
                return str(content_div)
            
            # 2. Bulamazsa, ana (dÄ±ÅŸ) 'icerik-govde' div'ini ara
            log_debug(f"âš ï¸ [{self.name}] Ä°Ã§-iÃ§e 'icerik-govde' bulunamadÄ±. Ana 'icerik-govde' aranacak: {url}")
            content_div = soup.find('div', class_='icerik-govde')
            if content_div:
                return str(content_div)
            
            # 3. O da bulunamazsa None dÃ¶ndÃ¼r.
            log_debug(f"âš ï¸ [{self.name}] 'icerik-govde' bulunamadÄ±. Hash iÃ§in baÅŸlÄ±k kullanÄ±lacak. URL: {url}")
            return None

        except Exception as e:
            log_error(f"âŒ [{self.name}] Duyuru iÃ§eriÄŸi Ã§ekilirken hata: {url}")
            log_error(f"âŒ [{self.name}] {e}")
            raise   # HatayÄ± _run_check'e (scheduler) geri fÄ±rlat
        

    def parse_announcements(self, soup: BeautifulSoup) -> List[Dict]:
        """
        [IMPLEMENTS BaseScraper]
        Ana liste sayfasÄ±nÄ±n HTML'ini (soup) parse ederek duyuru listesini Ã§Ä±karÄ±r.
        
        BÅEÃœ'nÃ¼n 'liste-gorunum' ID'li 'div'i iÃ§indeki 'tbody' > 'tr'
        yapÄ±sÄ±nÄ± baz alÄ±r.
        
        Args:
            soup: `fetch_page`'den gelen BeautifulSoup nesnesi.
            
        Returns:
            `BaseScraper` contract'Ä±na uygun duyuru listesi.
        """
        announcements_list = []
        
        # HTML'de "Liste" gÃ¶rÃ¼nÃ¼mÃ¼nÃ¼n ID'si 'liste-gorunum'
        list_view = soup.find('div', id='liste-gorunum')
        
        if not list_view:
            log_error(f"âŒ {self.name}: 'liste-gorunum' ID'li ana div bulunamadÄ±!")
            return []

        # Bu div iÃ§indeki tablo gÃ¶vdesini (tbody) bul
        table_body = list_view.find('tbody')
        
        if not table_body:
            log_error(f"âŒ {self.name}: 'tbody' elementi bulunamadÄ±!")
            return []
            
        # tbody iÃ§indeki tÃ¼m satÄ±rlarÄ± (tr) al
        rows = table_body.find_all('tr')
        
        if not rows:
            log_error(f"âŒ {self.name}: 'tbody' iÃ§inde 'tr' (satÄ±r) bulunamadÄ±!")
            return []

        # Her bir satÄ±rÄ± (duyuruyu) iÅŸle
        for row in rows:
            try:
                # SatÄ±rdaki tÃ¼m hÃ¼creleri (td) al
                cells = row.find_all('td')
                
                # Beklenen yapÄ±da en az 2 hÃ¼cre olmalÄ± (Tarih, BaÅŸlÄ±k)
                if len(cells) < 2:
                    continue
                    
                # 1. Tarihi al (ilk hÃ¼cre)
                #    BaseScraper'daki _clean_text'i kullan
                date = self._clean_text(cells[0].get_text())
                
                # 2. BaÅŸlÄ±k ve linki al (ikinci hÃ¼cre)
                link_tag = cells[1].find('a')
                
                if not link_tag:
                    continue # Link yoksa bu satÄ±rÄ± atla
                    
                # 3. Verileri ayÄ±kla
                title = self._clean_text(link_tag.get_text())
                relative_url = link_tag.get('href')
                
                if not relative_url:
                    continue # 'href' attribute'u boÅŸsa atla
                    
                # GÃ¶receli URL'i (Ã¶rn: /bilgisayar/Icerik/...) tam adrese Ã§evir
                full_url = urljoin(self.BASE_URL, relative_url)
                
                # URL'den ID Ã¼ret (BaseScraper'daki yardÄ±mcÄ± fonksiyon)
                # ID iÃ§in gÃ¶receli URL'i kullanmak daha temiz bir ID saÄŸlar
                ann_id = self._generate_id_from_url(relative_url)
                
                # 4. SÃ¶zlÃ¼ÄŸÃ¼ oluÅŸtur ve listeye ekle
                announcements_list.append({
                    'id': ann_id,
                    'title': title,
                    'url': full_url,
                    'date': date,
                })
                
            except Exception as e:
                log_warning(f"âš ï¸ {self.name}: Bir duyuru satÄ±rÄ± parse edilirken hata: {e}")
                continue
        
        return announcements_list