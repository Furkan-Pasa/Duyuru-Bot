# scrapers/base_scraper.py
"""
TÃ¼m scraper'lar iÃ§in soyut (abstract) temel sÄ±nÄ±f.

Bu sÄ±nÄ±f, tÃ¼m scraper'lar iÃ§in ortak olan ÅŸu iÅŸlevleri saÄŸlar:
- `requests.Session` yÃ¶netimi (performans ve ortak header'lar iÃ§in).
- `fetch_page`: SayfayÄ± `requests` ile Ã§ekme, 'retry' (yeniden deneme) mantÄ±ÄŸÄ±.
- `scrape`: Ana orkestrasyon metodu (fetch -> parse).
- `_clean_text`, `_generate_id_from_url` gibi yardÄ±mcÄ± (utility) metotlar.

Bu sÄ±nÄ±ftan miras alan (inherit eden) her alt sÄ±nÄ±f,
iki metodu @abstractmethod gereÄŸi EZMEK (override) zorundadÄ±r:
1. `parse_announcements`: Sayfa listesini (soup) alÄ±p duyuru listesi dÃ¶ndÃ¼rÃ¼r.
2. `fetch_announcement_content`: Tek bir duyuru URL'ine gidip iÃ§erik metnini/HTML'ini dÃ¶ndÃ¼rÃ¼r.
"""

import time
import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from abc import ABC, abstractmethod  # Soyut sÄ±nÄ±f iÃ§in

import bot_config
from core.logger import log_info, log_error, log_debug, log_scraper_error, log_scraper_success, log_warning


class BaseScraper(ABC):
    """
    Soyut temel scraper sÄ±nÄ±fÄ±.
    (YukarÄ±daki modÃ¼l docstring'ini inceleyin)
    """

    def __init__(self, url: str, name: str):
        """
        Scraper'Ä± baÅŸlatÄ±r ve HTTP session'Ä± oluÅŸturur.

        Args:
            url (str): Scrape edilecek ana liste URL'si.
            name (str): Scraper adÄ± (loglama ve DB iÃ§in).
        """
        self.url = url
        self.name = name
        # HTTP oturumunu (session) baÅŸlat
        self.session = self._create_session()
        log_debug(f"ğŸ”§ {self.name} scraper hazÄ±rlandÄ±")

    def _create_session(self) -> requests.Session:
        """
        Ortak header'larÄ± (User-Agent vb.) olan bir 'requests.Session' oluÅŸturur.
        
        Session kullanmak, TCP baÄŸlantÄ±larÄ±nÄ± yeniden kullanarak
        performansÄ± artÄ±rÄ±r (HTTP Keep-Alive).
        """
        session = requests.Session()

       # Standart tarayÄ±cÄ± gibi gÃ¶rÃ¼n (bot engeline takÄ±lmamak iÃ§in)
        session.headers.update({
            "User-Agent": bot_config.USER_AGENT,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        })

        return session

    def fetch_page(self) -> BeautifulSoup:
        """
        Ana duyuru listesi sayfasÄ±nÄ±n HTML'ini Ã§eker ve 'soup' dÃ¶ndÃ¼rÃ¼r.

        `bot_config`'deki 'MAX_RETRIES' ve 'RETRY_DELAY' ayarlarÄ±na gÃ¶re
        hata durumunda (Timeout, ConnectionError, HTTPError) yeniden dener.
        
        Returns:
            BeautifulSoup: Parse edilmiÅŸ HTML nesnesi.
            
        Raises:
            Exception: HTTP 4xx/5xx hatasÄ± veya max_retries aÅŸÄ±ldÄ±ÄŸÄ±nda.
        """
        retries = 0

        while retries < bot_config.MAX_RETRIES:
            try:
                # GÃ¼venlik duvarÄ±na takÄ±lmamak iÃ§in gecikme
                time.sleep(bot_config.REQUEST_DELAY_MS / 1000.0)
                
                log_debug(f"ğŸŒ [{self.name}] Sayfa Ã§ekiliyor... ({self.url})")

                response = self.session.get(
                    self.url, timeout=bot_config.REQUEST_TIMEOUT
                )

                # HTTP 4xx veya 5xx hata kodlarÄ± iÃ§in (Ã¶rn: 404, 500)
                response.raise_for_status()

                # Encoding kontrolÃ¼ (TÃ¼rkÃ§e karakter sorunlarÄ± iÃ§in)
                response.encoding = response.apparent_encoding

                # lxml parser ile parse et (hÄ±zlÄ±)
                soup = BeautifulSoup(response.text, "lxml")

                log_debug(f"âœ… {self.name}: Sayfa baÅŸarÄ±yla Ã§ekildi")
                return soup

            # Yeniden denenebilir hatalar (Timeout, BaÄŸlantÄ±, HTTP HatalarÄ±)
            except (requests.exceptions.Timeout, requests.exceptions.ConnectionError, requests.exceptions.HTTPError) as e:
                retries += 1
                log_warning(f"âš ï¸ [{self.name}] Sayfa Ã§ekme hatasÄ± ({e.__class__.__name__}). Tekrar deneniyor... ({retries}/{bot_config.MAX_RETRIES})")
                time.sleep(bot_config.RETRY_DELAY)

            except Exception as e:
                # DiÄŸer beklenmeyen (yeniden denenemez) hatalar
                log_error(f"âŒ [{self.name}] Sayfa Ã§ekilirken beklenmeyen hata: {e}", exc_info=True)
                raise # Bu hatayÄ± yeniden fÄ±rlat, scrape() yakalasÄ±n

        # While dÃ¶ngÃ¼sÃ¼ bittiyse (retries aÅŸÄ±ldÄ±)
        log_scraper_error(self.name, Exception(f"{bot_config.MAX_RETRIES} deneme sonrasÄ± sayfa Ã§ekilemedi!"))
        raise Exception(f"{self.name}: {bot_config.MAX_RETRIES} deneme sonrasÄ± sayfa Ã§ekilemedi!")
    
    @abstractmethod
    def parse_announcements(self, soup: BeautifulSoup) -> List[Dict]:
        """
        [ZORUNLU] HTML (soup) 'u parse ederek duyuru listesini dÃ¶ndÃ¼rÃ¼r.

        Bu metot, alt sÄ±nÄ±flar (Ã¶rn: BSEU_Duyuru.py) tarafÄ±ndan
        mutlaka ezilmeli (override) ve o sitenin HTML yapÄ±sÄ±na
        gÃ¶re yazÄ±lmalÄ±dÄ±r.

        Args:
            soup: `fetch_page`'den gelen BeautifulSoup nesnesi.

        Returns:
            Duyuru sÃ¶zlÃ¼klerinin listesi.
            SÃ¶zlÃ¼k yapÄ±sÄ± (contract) ÅŸÃ¶yle olmalÄ±:
            [
                {
                    'id': 'benzersiz_duyuru_id',
                    'title': 'Duyuru baÅŸlÄ±ÄŸÄ±',
                    'url': 'https://... (tam URL)',
                    'date': '01.01.2025' (opsiyonel)
                },
                ...
            ]
        """
        pass

    @abstractmethod
    def fetch_announcement_content(self, url: str) -> Optional[str]:
        """
        [ZORUNLU] Tek bir duyurunun URL'ine giderek iÃ§eriÄŸini Ã§eker.

        Bu metot, alt sÄ±nÄ±flar tarafÄ±ndan mutlaka ezilmelidir.
        DÃ¶ndÃ¼rdÃ¼ÄŸÃ¼ string, 'content_hash' oluÅŸturmak iÃ§in kullanÄ±lÄ±r.
        Genellikle duyurunun ana metnini/HTML'ini iÃ§eren
        div'in (Ã¶rn: 'div.icerik-govde') 'str()' hali olmalÄ±dÄ±r.

        Args:
            url: Tek bir duyurunun tam URL'i.

        Returns:
            str: Hash'lenecek iÃ§erik (genellikle ham HTML string'i).
        """
        pass

    def scrape(self) -> List[Dict]:
        """
        Ana scraping orkestrasyon metodu.
        
        SÄ±rasÄ±yla `fetch_page()` ve `parse_announcements()`'i Ã§aÄŸÄ±rÄ±r.
        TÃ¼m sÃ¼reci yÃ¶netir ve duyuru listesini dÃ¶ndÃ¼rÃ¼r.
        Scheduler bu metodu Ã§aÄŸÄ±rÄ±r.

        Returns:
            Duyuru listesi (veya hata durumunda boÅŸ liste).
        """
        try:
            # 1. SayfayÄ± Ã§ek (retry mantÄ±ÄŸÄ± iÃ§erir)
            soup = self.fetch_page()

            # 2. DuyurularÄ± parse et (alt sÄ±nÄ±fÄ±n mantÄ±ÄŸÄ±)
            announcements = self.parse_announcements(soup)

            log_scraper_success(self.name, len(announcements))
            return announcements

        except Exception as e:
            # fetch_page veya parse_announcements'dan gelen hatalar
            log_scraper_error(self.name, e)
            return [] # Hata durumunda boÅŸ liste dÃ¶ndÃ¼r, scheduler devam etsin

    def _clean_text(self, text: str) -> str:
        """
        YardÄ±mcÄ± metot: Metni (genellikle baÅŸlÄ±klarÄ±) temizler.
        Fazla boÅŸluklarÄ±, satÄ±r baÅŸlarÄ±nÄ± vb. kaldÄ±rÄ±r.

        Args:
            text: Ham metin (Ã¶rn: "\n\n   BaÅŸlÄ±k \r\n  ")

        Returns:
            TemizlenmiÅŸ metin (Ã¶rn: "BaÅŸlÄ±k")
        """
        if not text:
            return ""

        # 'str.split()' tÃ¼m whitespace'leri (space, \n, \t, \r) ayÄ±rÄ±r,
        # ' '.join() ile tek boÅŸlukla birleÅŸtirir.
        text = " ".join(text.split())

        # strip() sadece baÅŸtaki/sondakini alÄ±r, ama split/join daha garanti
        text = text.strip()

        return text

    def _generate_id_from_url(self, url: str) -> str:
        """
        YardÄ±mcÄ± metot: URL'den basit bir ID Ã¼retmeye Ã§alÄ±ÅŸÄ±r.

        Genellikle URL'nin son parÃ§asÄ±nÄ± (/.../Icerik/12345) alÄ±r.
        Bu, ID'nin net olarak verilmediÄŸi siteler iÃ§in bir 'fallback'tir.

        Args:
            url: Duyuru URL'i (gÃ¶receli veya tam olabilir).

        Returns:
            ID string (Ã¶rn: "12345").
        """
        # URL'nin sonundaki '/' (trailing slash) varsa kaldÄ±r
        # ve '/' karakterine gÃ¶re bÃ¶l
        parts = url.rstrip("/").split("/")
        
        # Son parÃ§ayÄ± ID olarak kullan
        return parts[-1] if parts else url

    def close(self):
        """
        Scraper'a ait 'requests.Session'Ä± kapatÄ±r.
        Scheduler'Ä±n 'shutdown' metodunda Ã§aÄŸrÄ±lÄ±r.
        """
        if self.session:
            self.session.close()
            log_info(f"ğŸ”’ {self.name}: Session kapatÄ±ldÄ±")
